{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import time\n",
    "import gc\n",
    "import contextlib\n",
    "\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from io import StringIO\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.mlflow_functions import train_and_log, train_and_log_keras\n",
    "from models.scorer import home_credit_scoring_fn, home_credit_scorer\n",
    "from models.scorer import home_credit_loss_fn_keras\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n",
    "\n",
    "client = mlflow.MlflowClient(tracking_uri=os.path.abspath(\"../mlruns/\"))\n",
    "\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.create_experiment(name=\"home_credit_model\")\n",
    "except mlflow.MlflowException:\n",
    "    mlflow.set_experiment(experiment_name=\"home_credit_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_pickle(filepath_or_buffer=\"../data/df_hc_nm.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>...</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_Cash X-Sell: low_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_Cash X-Sell: middle_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS household with interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS household without interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS industry with interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS industry without interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS mobile with interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS mobile without interest_mean</th>\n",
       "      <th>PREV_PRODUCT_COMBINATION_POS other with interest_mean</th>\n",
       "      <th>PREV_number_applications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>979992.0</td>\n",
       "      <td>27076.5</td>\n",
       "      <td>702000.0</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>-18724</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>1193580.0</td>\n",
       "      <td>35028.0</td>\n",
       "      <td>855000.0</td>\n",
       "      <td>0.025164</td>\n",
       "      <td>-17482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>288873.0</td>\n",
       "      <td>16258.5</td>\n",
       "      <td>238500.0</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>-13384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>-7974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TARGET  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
       "0        0             0                0             0          202500.0   \n",
       "26       0             0                0             0          112500.0   \n",
       "40       0             0                0             0          202500.0   \n",
       "42       0             0                1             0          135000.0   \n",
       "45       1             0                0             1           90000.0   \n",
       "\n",
       "    AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  \\\n",
       "0     406597.5      24700.5         351000.0                    0.018801   \n",
       "26    979992.0      27076.5         702000.0                    0.018029   \n",
       "40   1193580.0      35028.0         855000.0                    0.025164   \n",
       "42    288873.0      16258.5         238500.0                    0.007305   \n",
       "45    180000.0       9000.0         180000.0                    0.009334   \n",
       "\n",
       "    DAYS_BIRTH  ...  PREV_PRODUCT_COMBINATION_Cash X-Sell: low_mean  \\\n",
       "0        -9461  ...                                             0.0   \n",
       "26      -18724  ...                                             NaN   \n",
       "40      -17482  ...                                             0.0   \n",
       "42      -13384  ...                                             0.0   \n",
       "45       -7974  ...                                             0.0   \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_Cash X-Sell: middle_mean  \\\n",
       "0                                                 0.0   \n",
       "26                                                NaN   \n",
       "40                                                0.0   \n",
       "42                                                0.2   \n",
       "45                                                0.0   \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS household with interest_mean  \\\n",
       "0                                                 0.0           \n",
       "26                                                NaN           \n",
       "40                                                0.0           \n",
       "42                                                0.2           \n",
       "45                                                0.0           \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS household without interest_mean  \\\n",
       "0                                                 0.0              \n",
       "26                                                NaN              \n",
       "40                                                0.0              \n",
       "42                                                0.0              \n",
       "45                                                0.0              \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS industry with interest_mean  \\\n",
       "0                                                 0.0          \n",
       "26                                                NaN          \n",
       "40                                                0.0          \n",
       "42                                                0.0          \n",
       "45                                                0.0          \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS industry without interest_mean  \\\n",
       "0                                                 0.0             \n",
       "26                                                NaN             \n",
       "40                                                0.0             \n",
       "42                                                0.0             \n",
       "45                                                0.0             \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS mobile with interest_mean  \\\n",
       "0                                            0.000000        \n",
       "26                                                NaN        \n",
       "40                                           0.166667        \n",
       "42                                           0.400000        \n",
       "45                                           1.000000        \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS mobile without interest_mean  \\\n",
       "0                                                 0.0           \n",
       "26                                                NaN           \n",
       "40                                                0.0           \n",
       "42                                                0.0           \n",
       "45                                                0.0           \n",
       "\n",
       "    PREV_PRODUCT_COMBINATION_POS other with interest_mean  \\\n",
       "0                                                 1.0       \n",
       "26                                                NaN       \n",
       "40                                                0.0       \n",
       "42                                                0.0       \n",
       "45                                                0.0       \n",
       "\n",
       "    PREV_number_applications  \n",
       "0                        1.0  \n",
       "26                       NaN  \n",
       "40                       6.0  \n",
       "42                       5.0  \n",
       "45                       2.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.rename(columns={\"TARGET\": \"Payment_difficulties\"}, inplace=True)\n",
    "\n",
    "target_col = \"Payment_difficulties\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation and experimentation\n",
    "\n",
    "- Threshold optimization : The first xgboost run will be used to adjust the threshold of the scorer\n",
    "- Models : Ensemble methods, handle missing values and don't require scaling\n",
    "    - xGBoost\n",
    "    - CatBoost\n",
    "    - LightGBM\n",
    "- Optimization : Best method will be determined via benchmark\n",
    "    - Hyperopt\n",
    "- Logging via MlFlow :\n",
    "    - Metrics\n",
    "    - Custom metric tuning\n",
    "    - AUROC, confusion matrix\n",
    "    - Global feature importance\n",
    "    - Model\n",
    "    - Method of undersampling\n",
    "    - Version (handling of nans.)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df_model.drop(columns=target_col),\n",
    "    df_model[target_col],\n",
    "    test_size=0.3,\n",
    "    random_state=123\n",
    "    )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=0.25,\n",
    "    random_state=123\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 : Overfitting prevention\n",
    "- Overfitting on tree based classifiers is often a problem. The weights of the weak estimators might introduce some bias in the final results\n",
    "- There are several steps we can take to prevent overfitting :\n",
    "    - A hold out set for validation : Splitting the training data (cell above) in a validation set makes testing the model on **unseen** data, this makes evaluation of the model more neutral\n",
    "    - Regularization parameters : among the hyperparameters tested, some parameters are set specifically to avoid overfitting : `learning_rate`, L1 & L2 regularization for example, are aggressively set higher than default.\n",
    "    - CrossValidation : hyperopt supports crossvalidation, as does GridSearchCV (but hyperopt is way faster in this scenario). Hyperparameter tuning is crossvalidated with `cross_val_score` set to `cv=5`\n",
    "    - DNN specific : two layers of dropout (30% then 20%) disables random neurons to avoid overfitting\n",
    "    - DNN specific : the model features the validation scores and metrics (in addition to the train/test metrics). This tests the ability of the classifier on unseen data and allows us to see when overfitting occurs. We can thus select the best model based on the validation score and not the training score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 xGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will aslo serve as calibration for the threshold of the metric\n",
    "- We will use the optimized xGboost to predict the probabilities of classification and apply the threshold finder onto it\n",
    "- This might mean that xGboost will be reran twice \n",
    "\n",
    "### Hyperopt optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_threshold = 0.6004038524356866  # RUN 3\n",
    "# Confirmed final threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [52:45<00:00, 158.27s/trial, best loss: -4.8676429400265295]\n",
      "Hyperopt search time: 3165.2632431249367 seconds.\n"
     ]
    }
   ],
   "source": [
    "xgb_param_space = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(200, 800, 25, dtype=int)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.arange(3, 9, dtype=int)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -7, -3),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 0.1),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", -6, -3),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", -6, -3),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 0.9),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.3, 0.9),\n",
    "    \"nthread\": -1\n",
    "}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        min_child_weight=params[\"min_child_weight\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        nthread=params[\"nthread\"]\n",
    "    )\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=make_scorer(home_credit_scoring_fn, threshold=adjusted_threshold)\n",
    "        )\n",
    "\n",
    "    loss = -np.mean(scores)\n",
    "    return {\"loss\": loss, \"status\": \"ok\"}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "time_start_hopt = time.perf_counter()\n",
    "best = fmin(objective, space=xgb_param_space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "time_end_hopt = time.perf_counter()\n",
    "\n",
    "print(f\"Hyperopt search time: {time_end_hopt - time_start_hopt} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5664064462789224, 'learning_rate': 0.001980282806150775, 'max_depth': 4, 'min_child_weight': 5.300000000000001, 'n_estimators': 475, 'nthread': -1, 'reg_alpha': 0.045887098756441945, 'reg_lambda': 0.004368002328795769, 'subsample': 0.6719121773597623}\n"
     ]
    }
   ],
   "source": [
    "best_params_xgb = space_eval(xgb_param_space, best)\n",
    "gc.collect()\n",
    "print(best_params_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "eval_metric is not saved in Scikit-Learn meta.\n",
      "Successfully registered model 'xbgoost_hyperopt_best_cv'.\n",
      "2023/03/07 14:18:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: xbgoost_hyperopt_best_cv, version 1\n",
      "Created version '1' of model 'xbgoost_hyperopt_best_cv'.\n",
      "Registered model 'xbgoost_hyperopt_best_cv' already exists. Creating a new version of this model...\n",
      "2023/03/07 14:18:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: xbgoost_hyperopt_best_cv, version 2\n",
      "Created version '2' of model 'xbgoost_hyperopt_best_cv'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "best_params_xgb_log = {\n",
    "    \"colsample_bytree\": 0.5664064462789224,\n",
    "    \"learning_rate\": 0.001980282806150775,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 5.300000000000001,\n",
    "    \"n_estimators\": 475,\n",
    "    \"reg_alpha\": 0.045887098756441945,\n",
    "    \"reg_lambda\": 0.004368002328795769,\n",
    "    \"subsample\": 0.6719121773597623\n",
    "    }\n",
    "\n",
    "xgb_metrics, xgb_clf = train_and_log(\n",
    "    estimator=XGBClassifier,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    score_threshold=adjusted_threshold,\n",
    "    dataset_version=\"nans_kept_shap_reduced\",\n",
    "    imb_method=\"near_miss_one\",\n",
    "    na_thresh=0,\n",
    "    params=best_params_xgb_log,\n",
    "    model_name=\"xbgoost_hyperopt_best_cv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1010.60trial/s, best loss: -9.999999999773346]\n",
      "Best threshold: 0.6004038524356866\n",
      "Best score: 9.999999999773346\n"
     ]
    }
   ],
   "source": [
    "# Treshold adjusted, skipping\n",
    "\n",
    "# # Adjusting threshold - then redoing the search\n",
    "\n",
    "# y_proba = xgb_clf.predict_proba(X_val)\n",
    "\n",
    "# threshold_space = hp.uniform(\"threshold\", 0, 1)\n",
    "\n",
    "\n",
    "# def objective(threshold):\n",
    "#     # Convert predicted probabilities to binary labels using the threshold\n",
    "#     y_pred_binary = (y_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "#     # Calculate the score using the custom scoring function\n",
    "#     score = home_credit_scoring_fn(y_true=y_val, y_pred=y_pred_binary)\n",
    "\n",
    "#     return {\"loss\": -score, \"status\": \"ok\"}\n",
    "\n",
    "\n",
    "# # Run the hyperparameter optimization\n",
    "# best = fmin(objective, threshold_space, algo=tpe.suggest, max_evals=100)\n",
    "\n",
    "# # Get the best threshold and score\n",
    "# best_threshold = best[\"threshold\"]\n",
    "# best_score = -objective(best_threshold)[\"loss\"]\n",
    "\n",
    "# print(\"Best threshold:\", best_threshold)\n",
    "# print(\"Best score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 4.89476167980676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      4270\n",
      "           1       0.74      0.79      0.77      4412\n",
      "\n",
      "    accuracy                           0.76      8682\n",
      "   macro avg       0.76      0.76      0.76      8682\n",
      "weighted avg       0.76      0.76      0.76      8682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_proba = xgb_clf.predict_proba(X_val)\n",
    "y_val_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "val_score = home_credit_scoring_fn(y_val, y_pred=y_val_pred)\n",
    "\n",
    "print(f\"Validation score: {val_score}\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "\n",
    "- The report is based on a validation/hold-out set, this is data the classifier was not trained/tested on. This shows the ability of the model on unseen data.\n",
    "- The metrics are overall pretty good. As we are interested mainly on false positives, recall and f1 are the two metrics we want to keep an eye on. Both are above 0.7.\n",
    "- This also shows the influence of the custom metric on the results : the recall and F1 are both higher when classifying data labelled as 1 (i. e. : the client is a risk). The training on the custom metric introduces some bias, as requested. This classifier is particularly well trained to find client who would present a risk, at the cost of misindentifying some that do not present a high risk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Catboost :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:19<00:00, 42.95s/trial, best loss: -5.33357793702625] \n",
      "Hyperopt search time: 859.0927498750389 seconds.\n"
     ]
    }
   ],
   "source": [
    "catboost_param_space = {\n",
    "    \"iterations\": hp.choice(\"iterations\", np.arange(200, 800, 25, dtype=int)),\n",
    "    \"depth\": hp.choice(\"depth\", np.arange(3, 9, dtype=int)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -7, -3),\n",
    "    \"l2_leaf_reg\": hp.loguniform(\"l2_leaf_reg\", -6, -3),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 0.9),\n",
    "    \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.3, 0.9),\n",
    "    \"verbose\": False\n",
    "    }\n",
    "\n",
    "\n",
    "def objective_cb(params):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=params[\"iterations\"],\n",
    "        depth=params[\"depth\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bylevel=params[\"colsample_bylevel\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "        verbose=params[\"verbose\"],\n",
    "        thread_count=-1,\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=make_scorer(home_credit_scoring_fn, threshold=adjusted_threshold)\n",
    "        )\n",
    "    loss = -np.mean(scores)\n",
    "    return {\"loss\": loss, \"status\": \"ok\"}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "time_start_hopt = time.perf_counter()\n",
    "best_cb = fmin(objective_cb, space=catboost_param_space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "time_end_hopt = time.perf_counter()\n",
    "\n",
    "print(f\"Hyperopt search time: {time_end_hopt - time_start_hopt} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.6085039314472103, 'depth': 3, 'iterations': 250, 'l2_leaf_reg': 0.0028892223893425576, 'learning_rate': 0.0010208031321721773, 'subsample': 0.7210157471199539, 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "best_params_cb = space_eval(catboost_param_space, best_cb)\n",
    "print(best_params_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Successfully registered model 'cb_hyperopt_best_cv'.\n",
      "2023/03/07 14:35:06 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: cb_hyperopt_best_cv, version 1\n",
      "Created version '1' of model 'cb_hyperopt_best_cv'.\n",
      "Registered model 'cb_hyperopt_best_cv' already exists. Creating a new version of this model...\n",
      "2023/03/07 14:35:06 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: cb_hyperopt_best_cv, version 2\n",
      "Created version '2' of model 'cb_hyperopt_best_cv'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "best_params_cb_log = {\n",
    "    \"colsample_bylevel\": 0.6085039314472103,\n",
    "    \"depth\": 3,\n",
    "    \"iterations\": 250,\n",
    "    \"l2_leaf_reg\": 0.0028892223893425576,\n",
    "    \"learning_rate\": 0.0010208031321721773,\n",
    "    \"subsample\": 0.7210157471199539,\n",
    "    \"verbose\": False\n",
    "    }\n",
    "\n",
    "cb_metrics, cb_clf = train_and_log(\n",
    "    estimator=CatBoostClassifier,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    score_threshold=adjusted_threshold,\n",
    "    dataset_version=\"nans_kept_shap_reduced\",\n",
    "    imb_method=\"near_miss_one\",\n",
    "    na_thresh=0,\n",
    "    params=best_params_cb_log,\n",
    "    model_name=\"cb_hyperopt_best_cv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 5.213147410150871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70      4270\n",
      "           1       0.71      0.73      0.72      4412\n",
      "\n",
      "    accuracy                           0.71      8682\n",
      "   macro avg       0.71      0.71      0.71      8682\n",
      "weighted avg       0.71      0.71      0.71      8682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "\n",
    "y_val_proba_cb = cb_clf.predict_proba(X_val)\n",
    "y_val_pred_cb = cb_clf.predict(X_val)\n",
    "\n",
    "report_cb = classification_report(y_val, y_val_pred_cb)\n",
    "val_score_cb = home_credit_scoring_fn(y_val, y_val_pred_cb)\n",
    "\n",
    "print(f\"Validation score: {val_score_cb}\")\n",
    "print(report_cb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsevation :\n",
    "- Catboost seems to be less performant than xGboost, although still quite efficient. It exhibits the same behaviour (better at identifying false positives)\n",
    "- The model *might* perform better if the preprocessing wasnt featuring OneHotEncoding as CatBoost is specifically powerful on categorical data. However, the models should be tested on the same datasets for comparison purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 : Dense Neural Network :\n",
    "- DNNs perform better on scaled data (while ensemble methods are not impacted by scaling). The dataset used to train the DNN is the same as the one used by tree based classifiers, but with nans imputed and scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_scaled = pd.read_pickle(filepath_or_buffer=\"../data/df_train_hc_nm_imputed_scaled.pkl\")\n",
    "df_model_scaled.rename(columns={\"TARGET\": \"Payment_difficulties\"}, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redoing the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_model_scaled.drop(columns=[\"Payment_difficulties\"]),\n",
    "    df_model_scaled[\"Payment_difficulties\"],\n",
    "    test_size=0.3,\n",
    "    random_state=123,\n",
    "    stratify=df_model_scaled[\"Payment_difficulties\"]\n",
    "    )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=123,\n",
    "    stratify=y_train\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN : \n",
    "\n",
    "- input of size feature len\n",
    "- 512 dense\n",
    "- dropout 30%\n",
    "- 256 dense\n",
    "- dropout 20%\n",
    "- Sigmoid for binary clf\n",
    "- Validation set to track overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting loss function with threshold : \n",
    "\n",
    "def home_credit_loss_fn_keras(y_true, y_pred, threshold=adjusted_threshold):\n",
    "    \"\"\"\n",
    "    Custom loss function meant to modify binary crossentropy to use the costs and threshold of\n",
    "    the home_credit metrics, specific to keras\n",
    "    \"\"\"\n",
    "    fn_cost = 10\n",
    "    fp_cost = 1\n",
    "\n",
    "    y_pred_binary = K.cast(K.greater_equal(y_pred, threshold), \"float32\")\n",
    "\n",
    "    fp = K.sum(K.cast(K.equal(y_pred_binary, 1) & K.equal(y_true, 0), \"float32\"))\n",
    "    fn = K.sum(K.cast(K.equal(y_pred_binary, 0) & K.equal(y_true, 1), \"float32\"))\n",
    "\n",
    "    loss = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    loss = loss + (fn * fn_cost + fp * fp_cost) / (fn + fp + 1e-7)  # Prevents zero division\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tensorflow.keras.metrics.Recall()\n",
    "\n",
    "# Input layer\n",
    "inputs = tensorflow.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Hidden layers\n",
    "dense_1 = tensorflow.keras.layers.Dense(512, activation=\"relu\")(inputs)\n",
    "dropout_30 = tensorflow.keras.layers.Dropout(0.3)(dense_1)\n",
    "dense_2 = tensorflow.keras.layers.Dense(256, activation=\"relu\")(dropout_30)\n",
    "dropout_20 = tensorflow.keras.layers.Dropout(0.2)(dense_2)\n",
    "\n",
    "# Output layer\n",
    "outputs = tensorflow.keras.layers.Dense(1, activation=\"sigmoid\")(dropout_20)\n",
    "\n",
    "model = tensorflow.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=home_credit_loss_fn_keras, metrics=[recall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 283)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               145408    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,993\n",
      "Trainable params: 276,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_summary = model.summary()\n",
    "\n",
    "summary_buffer = StringIO()\n",
    "\n",
    "with contextlib.redirect_stdout(summary_buffer):\n",
    "    model.summary()\n",
    "\n",
    "model_summary = summary_buffer.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 14:35:53.027736: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 2s 3ms/step - loss: 7.8201 - recall: 0.7030 - val_loss: 9.1378 - val_recall: 0.6676\n",
      "Epoch 2/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.7566 - recall: 0.7487 - val_loss: 7.8400 - val_recall: 0.7431\n",
      "Epoch 3/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.6662 - recall: 0.7629 - val_loss: 7.7491 - val_recall: 0.7554\n",
      "Epoch 4/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.7017 - recall: 0.7687 - val_loss: 7.0881 - val_recall: 0.7982\n",
      "Epoch 5/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.5705 - recall: 0.7808 - val_loss: 7.0113 - val_recall: 0.7954\n",
      "Epoch 6/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.5005 - recall: 0.7960 - val_loss: 7.0905 - val_recall: 0.7717\n",
      "Epoch 7/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.5011 - recall: 0.8049 - val_loss: 7.2546 - val_recall: 0.7643\n",
      "Epoch 8/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.3623 - recall: 0.8129 - val_loss: 6.8341 - val_recall: 0.7915\n",
      "Epoch 9/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.3416 - recall: 0.8218 - val_loss: 7.5902 - val_recall: 0.7339\n",
      "Epoch 10/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.3528 - recall: 0.8327 - val_loss: 7.1835 - val_recall: 0.7441\n",
      "Epoch 11/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.2315 - recall: 0.8462 - val_loss: 6.5396 - val_recall: 0.7777\n",
      "Epoch 12/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.1618 - recall: 0.8579 - val_loss: 6.7003 - val_recall: 0.7770\n",
      "Epoch 13/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.1241 - recall: 0.8660 - val_loss: 7.3799 - val_recall: 0.7335\n",
      "Epoch 14/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.0856 - recall: 0.8797 - val_loss: 7.5745 - val_recall: 0.6948\n",
      "Epoch 15/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.0958 - recall: 0.8847 - val_loss: 7.4392 - val_recall: 0.7074\n",
      "Epoch 16/35\n",
      "407/407 [==============================] - 1s 4ms/step - loss: 7.0297 - recall: 0.8949 - val_loss: 7.8590 - val_recall: 0.6743\n",
      "Epoch 17/35\n",
      "407/407 [==============================] - 2s 4ms/step - loss: 7.0823 - recall: 0.9008 - val_loss: 7.1599 - val_recall: 0.7256\n",
      "Epoch 18/35\n",
      "407/407 [==============================] - 2s 4ms/step - loss: 6.9455 - recall: 0.9105 - val_loss: 6.6160 - val_recall: 0.7547\n",
      "Epoch 19/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 7.0237 - recall: 0.9138 - val_loss: 6.8525 - val_recall: 0.7344\n",
      "Epoch 20/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.9079 - recall: 0.9198 - val_loss: 7.2392 - val_recall: 0.7160\n",
      "Epoch 21/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.8476 - recall: 0.9259 - val_loss: 7.4229 - val_recall: 0.6950\n",
      "Epoch 22/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.8180 - recall: 0.9292 - val_loss: 6.4966 - val_recall: 0.7514\n",
      "Epoch 23/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.7629 - recall: 0.9337 - val_loss: 6.4685 - val_recall: 0.7521\n",
      "Epoch 24/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.8052 - recall: 0.9359 - val_loss: 7.0571 - val_recall: 0.7273\n",
      "Epoch 25/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.6083 - recall: 0.9393 - val_loss: 6.7848 - val_recall: 0.7367\n",
      "Epoch 26/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.6795 - recall: 0.9418 - val_loss: 7.2929 - val_recall: 0.7019\n",
      "Epoch 27/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.6062 - recall: 0.9441 - val_loss: 6.7033 - val_recall: 0.7441\n",
      "Epoch 28/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.4121 - recall: 0.9456 - val_loss: 6.6385 - val_recall: 0.7399\n",
      "Epoch 29/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.5460 - recall: 0.9473 - val_loss: 6.8195 - val_recall: 0.7349\n",
      "Epoch 30/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.5155 - recall: 0.9507 - val_loss: 6.8167 - val_recall: 0.7337\n",
      "Epoch 31/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.4326 - recall: 0.9544 - val_loss: 6.8563 - val_recall: 0.7353\n",
      "Epoch 32/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.3497 - recall: 0.9568 - val_loss: 6.8652 - val_recall: 0.7326\n",
      "Epoch 33/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.4141 - recall: 0.9558 - val_loss: 7.1724 - val_recall: 0.7072\n",
      "Epoch 34/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.2817 - recall: 0.9564 - val_loss: 6.8584 - val_recall: 0.7344\n",
      "Epoch 35/35\n",
      "407/407 [==============================] - 1s 3ms/step - loss: 6.3094 - recall: 0.9590 - val_loss: 7.0260 - val_recall: 0.7196\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=35,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "training_time = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(history):\n",
    "    \"\"\"\n",
    "    Returns the best Keras model based on validation loss from the training history.\n",
    "    \"\"\"\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    best_epoch = val_loss.index(min(val_loss))\n",
    "    best_model = tensorflow.keras.models.clone_model(model)\n",
    "    best_model.build(input_shape=X_train.shape[1:])\n",
    "    best_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=home_credit_loss_fn_keras,\n",
    "        metrics=[tensorflow.keras.metrics.Recall()]\n",
    "        )\n",
    "    best_model.set_weights(history.model.get_weights())\n",
    "    return best_model, best_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_epoch = get_best_model(history=history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 647us/step - loss: 7.0477 - recall_1: 0.7196\n",
      "Best model achieved after : 22 epochs\n"
     ]
    }
   ],
   "source": [
    "best_score = best_model.evaluate(x=X_val, y=y_val)[0]\n",
    "print(f\"Best model achieved after : {best_epoch} epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 0s 837us/step\n",
      "466/466 [==============================] - 0s 619us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/psemp/Documents/GitHub/ds_p7/env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "2023/03/07 14:44:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/3s/s8sp6jwn6qs02jfxbgjc7c_40000gn/T/tmphy3sq8a5/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/3s/s8sp6jwn6qs02jfxbgjc7c_40000gn/T/tmphy3sq8a5/model/data/model/assets\n",
      "2023/03/07 14:44:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/3s/s8sp6jwn6qs02jfxbgjc7c_40000gn/T/tmphy3sq8a5/model, flavor: tensorflow), fall back to return ['tensorflow==2.11.0']. Set logging level to DEBUG to see the full traceback.\n",
      "Successfully registered model 'DNN_TEST'.\n",
      "2023/03/07 14:44:38 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: DNN_TEST, version 1\n",
      "Created version '1' of model 'DNN_TEST'.\n",
      "Registered model 'DNN_TEST' already exists. Creating a new version of this model...\n",
      "2023/03/07 14:44:38 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: DNN_TEST, version 2\n",
      "Created version '2' of model 'DNN_TEST'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "metrics, model = train_and_log_keras(\n",
    "    model=best_model,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    training_time=training_time,\n",
    "    model_summary=model_summary,\n",
    "    history=history,\n",
    "    model_name=\"DNN\",\n",
    "    dataset_version=\"Nans_imputed_scaled\",\n",
    "    imb_method=\"nm1\",\n",
    "    home_credit_score=best_score,\n",
    "    feature_list=X_train.columns.tolist()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7413ceb39ad46ea0813b284866778c289877dcdb7cc0e15aac0f5b04eb145bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
