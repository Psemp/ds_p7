{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import time\n",
    "import gc\n",
    "import contextlib\n",
    "\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.mlflow_functions import train_and_log, train_and_log_keras\n",
    "from models.scorer import home_credit_scoring_fn, home_credit_scorer, hc_threshold_score\n",
    "from models.scorer import home_credit_loss_fn_keras\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n",
    "\n",
    "client = mlflow.MlflowClient(tracking_uri=os.path.abspath(\"../mlruns/\"))\n",
    "\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.create_experiment(name=\"home_credit_model\")\n",
    "except mlflow.MlflowException:\n",
    "    mlflow.set_experiment(experiment_name=\"home_credit_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_pickle(filepath_or_buffer=\"../data/df_hc_nm.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.rename(columns={\"TARGET\": \"Loan_granted\"}, inplace=True)\n",
    "\n",
    "target_col = \"Loan_granted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"Loan_granted\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Modelisation and experimentation\n",
    "\n",
    "- Threshold optimization : The first xgboost run will be used to adjust the threshold of the scorer\n",
    "- Models : Ensemble methods, handle missing values and don't require scaling\n",
    "    - xGBoost\n",
    "    - CatBoost\n",
    "    - LightGBM\n",
    "- Optimization : Best method will be determined via benchmark\n",
    "    - GridSearchCV\n",
    "    - Hyperopt\n",
    "- Logging via MlFlow :\n",
    "    - Metrics\n",
    "    - Custom metric tuning\n",
    "    - AUROC, confusion matrix\n",
    "    - Global feature importance\n",
    "    - Model\n",
    "    - Method of undersampling\n",
    "    - Version (handling of nans.)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df_model.drop(columns=target_col),\n",
    "    df_model[target_col],\n",
    "    test_size=0.3,\n",
    "    random_state=123\n",
    "    )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=0.25,\n",
    "    random_state=123\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 xGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_space = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(200, 800, 25, dtype=int)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.arange(3, 9, dtype=int)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -7, -3),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 0.1),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", -6, -3),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", -6, -3),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 0.9),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.3, 0.9),\n",
    "    \"nthread\": -1\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        min_child_weight=params[\"min_child_weight\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        nthread=params[\"nthread\"]\n",
    "    )\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=home_credit_scorer)\n",
    "    loss = -np.mean(scores)\n",
    "    return {\"loss\": loss, \"status\": \"ok\"}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "time_start_hopt = time.perf_counter()\n",
    "best = fmin(objective, space=xgb_param_space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "time_end_hopt = time.perf_counter()\n",
    "\n",
    "print(f\"Hyperopt search time: {time_end_hopt - time_start_hopt} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = space_eval(xgb_param_space, best)\n",
    "gc.collect()\n",
    "print(best_params_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "best_params_xgb_log = {\n",
    "    \"colsample_bytree\": 0.778655523535136,\n",
    "    \"learning_rate\": 0.0054998702766629335,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"n_estimators\": 700,\n",
    "    \"reg_alpha\": 0.037071627790320846,\n",
    "    \"reg_lambda\": 0.006294264780463717,\n",
    "    \"subsample\": 0.8644766597785274\n",
    "    }\n",
    "\n",
    "xgb_metrics, xgb_clf = train_and_log(\n",
    "    estimator=XGBClassifier,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    dataset_version=\"nans_kept_shap_reduced\",\n",
    "    imb_method=\"near_miss_one\",\n",
    "    na_thresh=0,\n",
    "    params=best_params_xgb_log,\n",
    "    model_name=\"xbgoost_metric_calibration_run\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There might be overfitting, we will try and adjust it\n",
    "\n",
    "y_proba = xgb_clf.predict_proba(X_val)\n",
    "\n",
    "optimal_threshold = hc_threshold_score(y_val, y_proba)\n",
    "\n",
    "y_pred = (y_proba[:, 1] >= optimal_threshold).astype(int)\n",
    "\n",
    "score = home_credit_scoring_fn(y_val, y_pred)\n",
    "print(\"Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking the high AUC with classification report\n",
    "\n",
    "y_val_proba = xgb_clf.predict_proba(X_val)\n",
    "y_val_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "val_score = home_credit_scoring_fn(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation score: {val_score}\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "With an AUC of .881, it might be considered that the model is overfitting - or the fact that the competition is quite old in a machine learning POV might explain why xGboost is having such high performances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 : Catboost :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_param_space = {\n",
    "    \"iterations\": hp.choice(\"iterations\", np.arange(200, 800, 25, dtype=int)),\n",
    "    \"depth\": hp.choice(\"depth\", np.arange(3, 9, dtype=int)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -7, -3),\n",
    "    \"l2_leaf_reg\": hp.loguniform(\"l2_leaf_reg\", -6, -3),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 0.9),\n",
    "    \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.3, 0.9),\n",
    "    \"thread_count\": -1\n",
    "    }\n",
    "\n",
    "\n",
    "def objective_cb(params):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=params[\"iterations\"],\n",
    "        depth=params[\"depth\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bylevel=params[\"colsample_bylevel\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=home_credit_scorer)\n",
    "    loss = -np.mean(scores)\n",
    "    return {\"loss\": loss, \"status\": \"ok\"}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "time_start_hopt = time.perf_counter()\n",
    "best_cb = fmin(objective_cb, space=catboost_param_space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "time_end_hopt = time.perf_counter()\n",
    "\n",
    "print(f\"Hyperopt search time: {time_end_hopt - time_start_hopt} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_cb = space_eval(catboost_param_space, best_cb)\n",
    "print(best_params_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "best_params_cb_log = {\n",
    "    \"colsample_bylevel\": 0.8492535750204921,\n",
    "    \"depth\": 7,\n",
    "    \"iterations\": 700,\n",
    "    \"l2_leaf_reg\": 0.011953394749318379,\n",
    "    \"learning_rate\": 0.018551685647746795,\n",
    "    \"subsample\": 0.8297130817489142,\n",
    "    \"verbose\": False\n",
    "    }\n",
    "\n",
    "cb_metrics, cb_clf = train_and_log(\n",
    "    estimator=CatBoostClassifier,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    dataset_version=\"nans_kept_shap_reduced\",\n",
    "    imb_method=\"near_miss_one\",\n",
    "    na_thresh=0,\n",
    "    params=best_params_cb_log,\n",
    "    model_name=\"catboost_cvalidated\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking the high AUC with classification report\n",
    "\n",
    "y_val_proba_cb = cb_clf.predict_proba(X_val)\n",
    "y_val_pred_cb = cb_clf.predict(X_val)\n",
    "\n",
    "report_cb = classification_report(y_val, y_val_pred_cb)\n",
    "val_score_cb = home_credit_scoring_fn(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation score: {val_score_cb}\")\n",
    "print(report_cb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The problem of our generic function doesnt account for catboost specific scoring args, will be addressed at the end of the nb by defining again the functions imported for mlflow_function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_imp = pd.read_pickle(filepath_or_buffer=\"../data/df_hc_nm_imputed.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_space = {\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -5, 0),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 8, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 2, 20, 1),\n",
    "    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 10, 1),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    \"max_features\": hp.choice(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "}\n",
    "\n",
    "def objective_gb(params):\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        n_estimators=int(params[\"n_estimators\"]),\n",
    "        max_depth=int(params[\"max_depth\"]),\n",
    "        min_samples_split=int(params[\"min_samples_split\"]),\n",
    "        min_samples_leaf=int(params[\"min_samples_leaf\"]),\n",
    "        subsample=params[\"subsample\"],\n",
    "        max_features=params[\"max_features\"],\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=home_credit_scorer)\n",
    "    loss = -np.mean(scores)\n",
    "    return {\"loss\": loss, \"status\": \"ok\"}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "time_start_hopt = time.perf_counter()\n",
    "best_gb = fmin(objective_gb, space=gb_param_space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "time_end_hopt = time.perf_counter()\n",
    "\n",
    "print(f\"Hyperopt search time: {time_end_hopt - time_start_hopt} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_gb = space_eval(catboost_param_space, best_gb)\n",
    "print(best_params_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "best_params_gb_log = {\n",
    "\n",
    "    }\n",
    "\n",
    "cb_metrics, cb_clf = train_and_log(\n",
    "    estimator=GradientBoostingClassifier,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    dataset_version=\"nans_removed_shap_reduced\",\n",
    "    imb_method=\"near_miss_one\",\n",
    "    na_thresh=0,\n",
    "    params=best_params_gb_log,\n",
    "    model_name=\"gradient_boost_clf_cv\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN :\n",
    "- Using the scaled version of the dataset without nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_scaled = pd.read_pickle(filepath_or_buffer=\"../data/df_hc_nm_imputed_scaled.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the numeric columns for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_model_scaled.drop(columns=[\"TARGET\"]),\n",
    "    df_model_scaled[\"TARGET\"],\n",
    "    test_size=0.3,\n",
    "    random_state=123,\n",
    "    stratify=df_model_scaled[\"TARGET\"]\n",
    "    )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=123,\n",
    "    stratify=y_train\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN : \n",
    "\n",
    "- input of size feature len\n",
    "- 512 dense\n",
    "- dropout 30%\n",
    "- 256 dense\n",
    "- dropout 20%\n",
    "- Sigmoid for binary clf\n",
    "- Validation set to track overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tensorflow.keras.metrics.Recall()\n",
    "\n",
    "# Input layer\n",
    "inputs = tensorflow.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Hidden layers\n",
    "dense_1 = tensorflow.keras.layers.Dense(512, activation=\"relu\")(inputs)\n",
    "dropout_30 = tensorflow.keras.layers.Dropout(0.3)(dense_1)\n",
    "dense_2 = tensorflow.keras.layers.Dense(256, activation=\"relu\")(dropout_30)\n",
    "dropout_20 = tensorflow.keras.layers.Dropout(0.2)(dense_2)\n",
    "\n",
    "# Output layer\n",
    "outputs = tensorflow.keras.layers.Dense(1, activation=\"sigmoid\")(dropout_20)\n",
    "\n",
    "model = tensorflow.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=home_credit_loss_fn_keras, metrics=[recall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = model.summary()\n",
    "\n",
    "summary_buffer = StringIO()\n",
    "\n",
    "with contextlib.redirect_stdout(summary_buffer):\n",
    "    model.summary()\n",
    "\n",
    "model_summary = summary_buffer.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=35,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "training_time = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(history):\n",
    "    \"\"\"\n",
    "    Returns the best Keras model based on validation loss from the training history.\n",
    "    \"\"\"\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    best_epoch = val_loss.index(min(val_loss))\n",
    "    best_model = tensorflow.keras.models.clone_model(model)\n",
    "    best_model.build(input_shape=X_train.shape[1:])\n",
    "    best_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=home_credit_loss_fn_keras,\n",
    "        metrics=[tensorflow.keras.metrics.Recall()]\n",
    "        )\n",
    "    best_model.set_weights(history.model.get_weights())\n",
    "    return best_model, best_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_epoch = get_best_model(history=history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = best_model.evaluate(x=X_val, y=y_val)[0]\n",
    "print(f\"Best model achieved after : {best_epoch} epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"home_credit_model\")\n",
    "\n",
    "metrics, model = train_and_log_keras(\n",
    "    model=best_model,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    training_time=training_time,\n",
    "    model_summary=model_summary,\n",
    "    history=history,\n",
    "    model_name=\"DNN_TEST\",\n",
    "    dataset_version=\"Nans_imputed_scaled\",\n",
    "    imb_method=\"nm1\",\n",
    "    home_credit_score=best_score,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7413ceb39ad46ea0813b284866778c289877dcdb7cc0e15aac0f5b04eb145bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
